#!/usr/bin/env python3
"""
vLLMæ€§èƒ½æŒ‡æ ‡å®¢æˆ·ç«¯
ç»“åˆOpenTelemetryè¿½è¸ªå’Œå®¢æˆ·ç«¯æµ‹é‡ï¼Œæä¾›å®Œæ•´çš„å•è¯·æ±‚æ€§èƒ½æŒ‡æ ‡

ç‰¹ç‚¹:
- æ”¯æŒå¹¶å‘è¯·æ±‚çš„å‡†ç¡®æŒ‡æ ‡è·å–
- åŒ…å«æ‰€æœ‰å…³é”®æŒ‡æ ‡: TTFT, TPOT, ITL, é˜Ÿåˆ—æ—¶é—´ç­‰
- åŸºäºW3C Trace Contextæ ‡å‡†ï¼Œæ— æŒ‡æ ‡æ··åˆé—®é¢˜
- ç”Ÿäº§å°±ç»ªï¼Œå¯ç›´æ¥ç”¨äºæ€§èƒ½ç›‘æ§

"""

import asyncio
import aiohttp
import json
import time
import uuid
import requests
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, field
import numpy as np

@dataclass
class RequestMetrics:
    """å•è¯·æ±‚çš„å®Œæ•´æ€§èƒ½æŒ‡æ ‡"""
    # åŸºç¡€ä¿¡æ¯
    request_id: str
    trace_id: str
    success: bool = False
    error_message: str = ""
    
    # æ—¶é—´ä¿¡æ¯
    send_time: Optional[float] = None                # è¯·æ±‚å‘é€æ—¶é—´æˆ³
    
    @property
    def send_time_iso(self) -> Optional[str]:
        """è·å–ISOæ ¼å¼çš„å‘é€æ—¶é—´"""
        if self.send_time:
            return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.send_time))
        return None
    
    # å®¢æˆ·ç«¯æµ‹é‡æŒ‡æ ‡ (æ¯«ç§’)
    client_e2e_latency: float = 0.0              # ç«¯åˆ°ç«¯å»¶è¿Ÿ
    client_ttft: Optional[float] = None          # é¦–æ¬¡tokenæ—¶é—´
    client_tpot: Optional[float] = None          # æ¯tokenå¹³å‡æ—¶é—´
    client_itl: Optional[float] = None           # tokené—´å¹³å‡å»¶è¿Ÿ
    
    # æœåŠ¡ç«¯æŒ‡æ ‡ (æ¯«ç§’, ä»OpenTelemetryè·å–)
    server_queue_time: Optional[float] = None    # é˜Ÿåˆ—ç­‰å¾…æ—¶é—´
    server_prefill_time: Optional[float] = None  # é¢„å¡«å……æ—¶é—´
    server_decode_time: Optional[float] = None   # è§£ç æ—¶é—´
    server_inference_time: Optional[float] = None # æ¨ç†æ€»æ—¶é—´
    server_e2e_time: Optional[float] = None      # æœåŠ¡ç«¯E2Eæ—¶é—´
    server_ttft: Optional[float] = None          # æœåŠ¡ç«¯TTFT
    
    
    # Tokenç»Ÿè®¡
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    
    # è¯·æ±‚å‚æ•°
    model_name: Optional[str] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    max_tokens: Optional[int] = None
    
    # å†…å®¹
    generated_text: str = ""
    
    # å®¢æˆ·ç«¯è¯¦ç»†æ•°æ®
    client_itl_list: List[float] = field(default_factory=list)  # å®Œæ•´çš„tokené—´å»¶è¿Ÿåˆ—è¡¨
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "request_id": self.request_id,
            "trace_id": self.trace_id,
            "success": self.success,
            "error_message": self.error_message,
            "timestamp": self.send_time,
            "send_time_iso": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.send_time)) if self.send_time else None,
            "client_metrics": {
                "e2e_latency_ms": self.client_e2e_latency,
                "ttft_ms": self.client_ttft,
                "tpot_ms": self.client_tpot,
                "itl_ms": self.client_itl,
            },
            "server_metrics": {
                "queue_time_ms": self.server_queue_time,
                "prefill_time_ms": self.server_prefill_time,
                "decode_time_ms": self.server_decode_time,
                "inference_time_ms": self.server_inference_time,
                "e2e_time_ms": self.server_e2e_time,
                "ttft_ms": self.server_ttft,
                
            },
            "tokens": {
                "prompt_tokens": self.prompt_tokens,
                "completion_tokens": self.completion_tokens,
            },
            "request_params": {
                "model": self.model_name,
                "temperature": self.temperature,
                "top_p": self.top_p,
                "max_tokens": self.max_tokens,
            },
            "content": {
                "generated_text": self.generated_text,
                "text_length": len(self.generated_text),
            },
            "detailed_data": {
                "itl_list_seconds": self.client_itl_list,  # åŸå§‹ç§’ä¸ºå•ä½çš„ITLåˆ—è¡¨
                "itl_list_ms": [itl * 1000 for itl in self.client_itl_list] if self.client_itl_list else [],  # æ¯«ç§’ä¸ºå•ä½çš„ITLåˆ—è¡¨
            }
        }

class VLLMMetricsClient:
    """vLLMæ€§èƒ½æŒ‡æ ‡å®¢æˆ·ç«¯"""
    
    def __init__(
        self, 
        vllm_base_url: str = "http://localhost:8000",
        jaeger_base_url: str = "http://localhost:16686",
        otlp_endpoint: str = "http://localhost:4317"
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            vllm_base_url: vLLMæœåŠ¡çš„åŸºç¡€URL
            jaeger_base_url: Jaeger UIçš„åŸºç¡€URL
            otlp_endpoint: OpenTelemetry Collectorçš„OTLPç«¯ç‚¹
        """
        self.vllm_base_url = vllm_base_url
        self.jaeger_base_url = jaeger_base_url
        self.otlp_endpoint = otlp_endpoint
        self.setup_tracing()
        
    def setup_tracing(self):
        """è®¾ç½®OpenTelemetryè¿½è¸ª"""
        try:
            resource = Resource.create({
                "service.name": "vllm-metrics-client",
                "service.version": "1.0.0"
            })
            
            provider = TracerProvider(resource=resource)
            otlp_exporter = OTLPSpanExporter(
                endpoint=self.otlp_endpoint,
                insecure=True
            )
            
            span_processor = BatchSpanProcessor(otlp_exporter)
            provider.add_span_processor(span_processor)
            trace.set_tracer_provider(provider)
            self.tracer = trace.get_tracer(__name__)
        except Exception:
            # å¦‚æœOpenTelemetryè®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨ç©ºçš„tracer
            self.tracer = None
    
    def _generate_trace_context(self) -> Dict[str, str]:
        """ç”Ÿæˆå”¯ä¸€çš„trace context"""
        trace_id = str(uuid.uuid4()).replace('-', '')[:32]
        span_id = str(uuid.uuid4()).replace('-', '')[:16]
        traceparent = f"00-{trace_id}-{span_id}-01"
        
        return {
            "traceparent": traceparent,
            "trace_id": trace_id,
            "span_id": span_id
        }
    
    async def send_request(
        self,
        messages: List[Dict[str, str]],
        model: str = "auto",
        temperature: float = 0.7,
        max_tokens: int = 100,
        top_p: float = 1.0,
        stream: bool = True,
        **kwargs
    ) -> RequestMetrics:
        """
        å‘é€è¯·æ±‚å¹¶è·å–å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼Œ"auto"è¡¨ç¤ºä½¿ç”¨æœåŠ¡ç«¯é»˜è®¤æ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            top_p: Top-pé‡‡æ ·å‚æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
            **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°
            
        Returns:
            RequestMetrics: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„ç»“æœå¯¹è±¡
        """
        request_id = str(uuid.uuid4())
        trace_context = self._generate_trace_context()
        
        metrics = RequestMetrics(
            request_id=request_id,
            trace_id=trace_context["trace_id"],
            send_time=time.time()
        )
        
        try:
            start_time = time.perf_counter()
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "messages": messages,
                "stream": stream,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": top_p,
                **kwargs
            }
            
            # åªåœ¨éautoæ¨¡å¼ä¸‹è®¾ç½®model
            if model != "auto":
                request_data["model"] = model
            
            # è®¾ç½®headers
            headers = {
                "Content-Type": "application/json",
                "traceparent": trace_context["traceparent"],
                "X-Request-Id": request_id
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.vllm_base_url}/v1/chat/completions",
                    json=request_data,
                    headers=headers
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        metrics.error_message = f"HTTP {response.status}: {error_text}"
                        return metrics
                    
                    # å¤„ç†å“åº”
                    if stream:
                        await self._process_streaming_response(response, metrics, start_time)
                    else:
                        await self._process_non_streaming_response(response, metrics, start_time)
                    
                    metrics.success = True
                    
        except Exception as e:
            metrics.error_message = str(e)
        
        return metrics
    
    async def _process_streaming_response(self, response, metrics: RequestMetrics, start_time: float):
        """å¤„ç†æµå¼å“åº”"""
        generated_text = ""
        first_token_time = None
        inter_token_latencies = []
        most_recent_timestamp = start_time
        
        async for line in response.content:
            line = line.decode('utf-8').strip()
            if line.startswith("data: "):
                data_str = line[6:]
                if data_str == "[DONE]":
                    break
                
                try:
                    data = json.loads(data_str)
                    if data["choices"][0]["delta"].get("content"):
                        current_time = time.perf_counter()
                        
                        if first_token_time is None:
                            first_token_time = current_time
                            metrics.client_ttft = (current_time - start_time) * 1000
                        else:
                            itl = current_time - most_recent_timestamp
                            inter_token_latencies.append(itl)
                        
                        generated_text += data["choices"][0]["delta"]["content"]
                        most_recent_timestamp = current_time
                        
                except json.JSONDecodeError:
                    continue
        
        end_time = time.perf_counter()
        
        # è®¡ç®—æŒ‡æ ‡
        metrics.client_e2e_latency = (end_time - start_time) * 1000
        metrics.generated_text = generated_text
        metrics.client_itl_list = inter_token_latencies
        
        if inter_token_latencies:
            metrics.client_tpot = (sum(inter_token_latencies) / len(inter_token_latencies)) * 1000
            metrics.client_itl = metrics.client_tpot
    
    async def _process_non_streaming_response(self, response, metrics: RequestMetrics, start_time: float):
        """å¤„ç†éæµå¼å“åº”"""
        response_data = await response.json()
        end_time = time.perf_counter()
        
        metrics.client_e2e_latency = (end_time - start_time) * 1000
        
        if "choices" in response_data and response_data["choices"]:
            metrics.generated_text = response_data["choices"][0]["message"]["content"]
    
    def get_server_metrics(self, trace_id: str, timeout: int = 10) -> bool:
        """
        ä»OpenTelemetryè·å–æœåŠ¡ç«¯æŒ‡æ ‡
        
        Args:
            trace_id: è¿½è¸ªID
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè·å–æŒ‡æ ‡
        """
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"{self.jaeger_base_url}/api/traces/{trace_id}")
                if response.status_code == 200:
                    trace_data = response.json()
                    return self._extract_server_metrics_from_trace(trace_data)
            except Exception:
                pass
            
            time.sleep(0.5)
        
        return False
    
    def _extract_server_metrics_from_trace(self, trace_data: Dict) -> Dict[str, Any]:
        """ä»traceæ•°æ®ä¸­æå–æœåŠ¡ç«¯æŒ‡æ ‡"""
        try:
            for trace in trace_data.get("data", []):
                for span in trace.get("spans", []):
                    if span.get("operationName") == "llm_request":
                        tags = {tag["key"]: tag["value"] for tag in span.get("tags", [])}
                        return {
                            "queue_time": self._safe_float_ms(tags.get("gen_ai.latency.time_in_queue")),
                            "prefill_time": self._safe_float_ms(tags.get("gen_ai.latency.time_in_model_prefill")),
                            "decode_time": self._safe_float_ms(tags.get("gen_ai.latency.time_in_model_decode")),
                            "inference_time": self._safe_float_ms(tags.get("gen_ai.latency.time_in_model_inference")),
                            "e2e_time": self._safe_float_ms(tags.get("gen_ai.latency.e2e")),
                            "ttft": self._safe_float_ms(tags.get("gen_ai.latency.time_to_first_token")),
                            
                            "prompt_tokens": self._safe_int(tags.get("gen_ai.usage.prompt_tokens")),
                            "completion_tokens": self._safe_int(tags.get("gen_ai.usage.completion_tokens")),
                            "model": tags.get("gen_ai.response.model"),
                            "temperature": self._safe_float(tags.get("gen_ai.request.temperature")),
                            "top_p": self._safe_float(tags.get("gen_ai.request.top_p")),
                            "max_tokens": self._safe_int(tags.get("gen_ai.request.max_tokens")),
                        }
        except Exception:
            pass
        
        return {}
    
    def enrich_with_server_metrics(self, metrics: RequestMetrics, timeout: int = 10) -> bool:
        """
        ç”¨æœåŠ¡ç«¯æŒ‡æ ‡ä¸°å¯ŒRequestMetricså¯¹è±¡
        
        Args:
            metrics: è¦ä¸°å¯Œçš„æŒ‡æ ‡å¯¹è±¡
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè·å–å¹¶è®¾ç½®æœåŠ¡ç«¯æŒ‡æ ‡
        """
        server_data = {}
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"{self.jaeger_base_url}/api/traces/{metrics.trace_id}")
                if response.status_code == 200:
                    trace_data = response.json()
                    server_data = self._extract_server_metrics_from_trace(trace_data)
                    if server_data:
                        break
            except Exception:
                pass
            
            time.sleep(0.5)
        
        if not server_data:
            return False
        
        # è®¾ç½®æœåŠ¡ç«¯æŒ‡æ ‡
        metrics.server_queue_time = server_data.get("queue_time")
        metrics.server_prefill_time = server_data.get("prefill_time")
        metrics.server_decode_time = server_data.get("decode_time")
        metrics.server_inference_time = server_data.get("inference_time")
        metrics.server_e2e_time = server_data.get("e2e_time")
        metrics.server_ttft = server_data.get("ttft")
        
        metrics.prompt_tokens = server_data.get("prompt_tokens")
        metrics.completion_tokens = server_data.get("completion_tokens")
        metrics.model_name = server_data.get("model")
        metrics.temperature = server_data.get("temperature")
        metrics.top_p = server_data.get("top_p")
        metrics.max_tokens = server_data.get("max_tokens")
        
        return True
    
    def _safe_float_ms(self, value) -> Optional[float]:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶è½¬æ¢ä¸ºæ¯«ç§’"""
        if value is not None:
            try:
                return float(value) * 1000
            except:
                pass
        return None
    
    def _safe_float(self, value) -> Optional[float]:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        if value is not None:
            try:
                return float(value)
            except:
                pass
        return None
    
    def _safe_int(self, value) -> Optional[int]:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        if value is not None:
            try:
                return int(value)
            except:
                pass
        return None
    
    def get_jaeger_url(self, trace_id: str) -> str:
        """è·å–Jaeger traceæŸ¥çœ‹URL"""
        return f"{self.jaeger_base_url}/trace/{trace_id}"

def format_metrics(metrics: RequestMetrics, show_details: bool = True) -> str:
    """
    æ ¼å¼åŒ–æ˜¾ç¤ºæŒ‡æ ‡
    
    Args:
        metrics: æŒ‡æ ‡å¯¹è±¡
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        str: æ ¼å¼åŒ–çš„æŒ‡æ ‡å­—ç¬¦ä¸²
    """
    if not metrics.success:
        return f"âŒ è¯·æ±‚å¤±è´¥: {metrics.error_message}"
    
    lines = [
        "="*80,
        "ğŸ“Š vLLMè¯·æ±‚æ€§èƒ½æŒ‡æ ‡",
        "="*80,
        f"ğŸ†” è¯·æ±‚ID: {metrics.request_id}",
        f"ğŸ”— Trace ID: {metrics.trace_id}",
        f"âœ… çŠ¶æ€: æˆåŠŸ",
    ]
    
    if metrics.model_name:
        lines.append(f"ğŸ¤– æ¨¡å‹: {metrics.model_name}")
    
    lines.extend([
        "",
        "â±ï¸  å»¶è¿ŸæŒ‡æ ‡ (æ¯«ç§’):",
        f"  å®¢æˆ·ç«¯E2Eå»¶è¿Ÿ:     {metrics.client_e2e_latency:.2f}ms"
    ])
    
    if metrics.server_e2e_time:
        lines.append(f"  æœåŠ¡ç«¯E2Eå»¶è¿Ÿ:     {metrics.server_e2e_time:.2f}ms")
    
    lines.append("")
    lines.append("ğŸš€ TTFT (é¦–æ¬¡Tokenæ—¶é—´):")
    if metrics.client_ttft:
        lines.append(f"  å®¢æˆ·ç«¯TTFT:        {metrics.client_ttft:.2f}ms")
    if metrics.server_ttft:
        lines.append(f"  æœåŠ¡ç«¯TTFT:        {metrics.server_ttft:.2f}ms")
    
    lines.append("")
    lines.append("âš¡ Tokenç”ŸæˆæŒ‡æ ‡:")
    if metrics.client_tpot:
        lines.append(f"  å®¢æˆ·ç«¯TPOT:        {metrics.client_tpot:.2f}ms")
    if metrics.client_itl:
        lines.append(f"  ITL (ä»¤ç‰Œé—´å»¶è¿Ÿ):   {metrics.client_itl:.2f}ms")
    
    if any([metrics.server_queue_time, metrics.server_prefill_time, 
            metrics.server_decode_time, metrics.server_inference_time]):
        lines.append("")
        lines.append("ğŸ–¥ï¸  æœåŠ¡ç«¯è¯¦ç»†æŒ‡æ ‡:")
        if metrics.server_queue_time is not None:
            lines.append(f"  é˜Ÿåˆ—ç­‰å¾…æ—¶é—´:       {metrics.server_queue_time:.2f}ms")
        if metrics.server_prefill_time:
            lines.append(f"  é¢„å¡«å……æ—¶é—´:         {metrics.server_prefill_time:.2f}ms")
        if metrics.server_decode_time:
            lines.append(f"  è§£ç æ—¶é—´:           {metrics.server_decode_time:.2f}ms")
        if metrics.server_inference_time:
            lines.append(f"  æ¨ç†æ€»æ—¶é—´:         {metrics.server_inference_time:.2f}ms")
        
    
    if metrics.prompt_tokens or metrics.completion_tokens:
        lines.append("")
        lines.append("ğŸ”¢ Tokenç»Ÿè®¡:")
        if metrics.prompt_tokens:
            lines.append(f"  è¾“å…¥tokens:        {metrics.prompt_tokens}")
        if metrics.completion_tokens:
            lines.append(f"  è¾“å‡ºtokens:        {metrics.completion_tokens}")
        if metrics.client_itl_list:
            lines.append(f"  ITLæ•°æ®ç‚¹:         {len(metrics.client_itl_list)} ä¸ª")
    
    if any([metrics.temperature, metrics.top_p, metrics.max_tokens]):
        lines.append("")
        lines.append("âš™ï¸  è¯·æ±‚å‚æ•°:")
        if metrics.temperature is not None:
            lines.append(f"  æ¸©åº¦:              {metrics.temperature}")
        if metrics.top_p is not None:
            lines.append(f"  Top-p:             {metrics.top_p}")
        if metrics.max_tokens:
            lines.append(f"  æœ€å¤§tokens:        {metrics.max_tokens}")
    
    if show_details and metrics.generated_text:
        lines.append("")
        content = metrics.generated_text[:200] + "..." if len(metrics.generated_text) > 200 else metrics.generated_text
        lines.append(f"ğŸ“„ ç”Ÿæˆå†…å®¹: {content}")
    
    lines.append("")
    lines.append(f"ğŸ”— Jaegeré“¾æ¥: http://localhost:16686/trace/{metrics.trace_id}")
    lines.append("="*80)
    
    return "\n".join(lines)


@dataclass
class BenchmarkSummary:
    """åŸºå‡†æµ‹è¯•æ±‡æ€»ç»Ÿè®¡"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    benchmark_duration: float = 0.0
    
    # Tokenç»Ÿè®¡
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    
    # ååé‡æŒ‡æ ‡
    request_throughput: float = 0.0
    output_token_throughput: float = 0.0
    total_token_throughput: float = 0.0
    
    # TTFTç»Ÿè®¡ (æ¯«ç§’)
    ttft_values: List[float] = field(default_factory=list)
    mean_ttft: float = 0.0
    median_ttft: float = 0.0
    p99_ttft: float = 0.0
    
    # TPOTç»Ÿè®¡ (æ¯«ç§’)
    tpot_values: List[float] = field(default_factory=list)
    mean_tpot: float = 0.0
    median_tpot: float = 0.0
    p99_tpot: float = 0.0
    
    # ITLç»Ÿè®¡ (æ¯«ç§’)
    itl_values: List[float] = field(default_factory=list)
    mean_itl: float = 0.0
    median_itl: float = 0.0
    p99_itl: float = 0.0
    
    # é˜Ÿåˆ—æ—¶é—´ç»Ÿè®¡ (æ¯«ç§’)
    queue_time_values: List[float] = field(default_factory=list)
    mean_queue_time: float = 0.0
    median_queue_time: float = 0.0
    p99_queue_time: float = 0.0


def calculate_benchmark_summary(results: List[RequestMetrics], start_time: float, end_time: float) -> BenchmarkSummary:
    """è®¡ç®—åŸºå‡†æµ‹è¯•æ±‡æ€»ç»Ÿè®¡"""
    summary = BenchmarkSummary()
    
    # åŸºæœ¬ç»Ÿè®¡
    summary.total_requests = len(results)
    summary.successful_requests = sum(1 for r in results if r.success)
    summary.failed_requests = summary.total_requests - summary.successful_requests
    summary.benchmark_duration = end_time - start_time
    
    # åªç»Ÿè®¡æˆåŠŸçš„è¯·æ±‚
    successful_results = [r for r in results if r.success]
    
    if not successful_results:
        return summary
    
    # Tokenç»Ÿè®¡
    summary.total_input_tokens = sum(r.prompt_tokens or 0 for r in successful_results)
    summary.total_output_tokens = sum(r.completion_tokens or 0 for r in successful_results)
    
    # ååé‡è®¡ç®—
    if summary.benchmark_duration > 0:
        summary.request_throughput = summary.successful_requests / summary.benchmark_duration
        summary.output_token_throughput = summary.total_output_tokens / summary.benchmark_duration
        summary.total_token_throughput = (summary.total_input_tokens + summary.total_output_tokens) / summary.benchmark_duration
    
    # æ”¶é›†å„ç§å»¶è¿Ÿæ•°æ®
    ttft_values = []
    tpot_values = []
    itl_values = []
    queue_time_values = []
    
    for result in successful_results:
        # TTFT (ä½¿ç”¨å®¢æˆ·ç«¯æˆ–æœåŠ¡ç«¯æ•°æ®)
        if result.client_ttft is not None:
            ttft_values.append(result.client_ttft)
        elif result.server_ttft is not None:
            ttft_values.append(result.server_ttft)
        
        # TPOT
        if result.client_tpot is not None:
            tpot_values.append(result.client_tpot)
        
        # ITL (ä½¿ç”¨ITLåˆ—è¡¨ä¸­çš„æ‰€æœ‰å€¼)
        if result.client_itl_list:
            itl_values.extend([itl * 1000 for itl in result.client_itl_list])  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # é˜Ÿåˆ—æ—¶é—´
        if result.server_queue_time is not None:
            queue_time_values.append(result.server_queue_time)
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    def calculate_stats(values):
        if not values:
            return 0.0, 0.0, 0.0
        arr = np.array(values)
        return float(np.mean(arr)), float(np.median(arr)), float(np.percentile(arr, 99))
    
    # TTFTç»Ÿè®¡
    summary.ttft_values = ttft_values
    summary.mean_ttft, summary.median_ttft, summary.p99_ttft = calculate_stats(ttft_values)
    
    # TPOTç»Ÿè®¡
    summary.tpot_values = tpot_values
    summary.mean_tpot, summary.median_tpot, summary.p99_tpot = calculate_stats(tpot_values)
    
    # ITLç»Ÿè®¡
    summary.itl_values = itl_values
    summary.mean_itl, summary.median_itl, summary.p99_itl = calculate_stats(itl_values)
    
    # é˜Ÿåˆ—æ—¶é—´ç»Ÿè®¡
    summary.queue_time_values = queue_time_values
    summary.mean_queue_time, summary.median_queue_time, summary.p99_queue_time = calculate_stats(queue_time_values)
    
    return summary


def format_benchmark_summary(summary: BenchmarkSummary) -> str:
    """æ ¼å¼åŒ–åŸºå‡†æµ‹è¯•æ±‡æ€»æŠ¥å‘Š"""
    lines = [
        "="*50,
        " "*10 + "Serving Benchmark Result" + " "*10,
        "="*50,
        f"Successful requests:                     {summary.successful_requests:<10}",
        f"Failed requests:                         {summary.failed_requests:<10}",
        f"Benchmark duration (s):                  {summary.benchmark_duration:<10.2f}",
        f"Total input tokens:                      {summary.total_input_tokens:<10}",
        f"Total generated tokens:                  {summary.total_output_tokens:<10}",
        f"Request throughput (req/s):              {summary.request_throughput:<10.2f}",
        f"Output token throughput (tok/s):         {summary.output_token_throughput:<10.2f}",
        f"Total Token throughput (tok/s):          {summary.total_token_throughput:<10.2f}",
    ]
    
    # TTFTç»Ÿè®¡
    if summary.ttft_values:
        lines.extend([
            "-"*15 + "Time to First Token" + "-"*16,
            f"Mean TTFT (ms):                          {summary.mean_ttft:<10.2f}",
            f"Median TTFT (ms):                        {summary.median_ttft:<10.2f}",
            f"P99 TTFT (ms):                           {summary.p99_ttft:<10.2f}",
        ])
    
    # TPOTç»Ÿè®¡
    if summary.tpot_values:
        lines.extend([
            "-"*5 + "Time per Output Token (excl. 1st token)" + "-"*6,
            f"Mean TPOT (ms):                          {summary.mean_tpot:<10.2f}",
            f"Median TPOT (ms):                        {summary.median_tpot:<10.2f}",
            f"P99 TPOT (ms):                           {summary.p99_tpot:<10.2f}",
        ])
    
    # ITLç»Ÿè®¡
    if summary.itl_values:
        lines.extend([
            "-"*15 + "Inter-token Latency" + "-"*16,
            f"Mean ITL (ms):                           {summary.mean_itl:<10.2f}",
            f"Median ITL (ms):                         {summary.median_itl:<10.2f}",
            f"P99 ITL (ms):                            {summary.p99_itl:<10.2f}",
        ])
    
    # é˜Ÿåˆ—æ—¶é—´ç»Ÿè®¡
    if summary.queue_time_values:
        lines.extend([
            "-"*18 + "Queue Time" + "-"*19,
            f"Mean Queue Time (ms):                    {summary.mean_queue_time:<10.2f}",
            f"Median Queue Time (ms):                  {summary.median_queue_time:<10.2f}",
            f"P99 Queue Time (ms):                     {summary.p99_queue_time:<10.2f}",
        ])
    
    lines.append("="*50)
    
    return "\n".join(lines)


# =============================================================================
# æ•°æ®ä¿å­˜åŠŸèƒ½
# =============================================================================

def save_request_metrics(metrics: RequestMetrics, filename: str, format: str = "json") -> bool:
    """
    ä¿å­˜å•ä¸ªè¯·æ±‚çš„æŒ‡æ ‡æ•°æ®åˆ°æ–‡ä»¶
    
    Args:
        metrics: RequestMetricså¯¹è±¡
        filename: æ–‡ä»¶å (ä¸å«æ‰©å±•å)
        format: æ–‡ä»¶æ ¼å¼ ("json" æˆ– "csv")
    
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    import json
    import os
    from datetime import datetime
    
    try:
        if format.lower() == "json":
            # JSONæ ¼å¼ä¿å­˜
            filepath = f"{filename}.json"
            data = metrics.to_dict()
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… è¯·æ±‚æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            return True
            
        elif format.lower() == "csv":
            # CSVæ ¼å¼ä¿å­˜
            import csv
            filepath = f"{filename}.csv"
            
            # å±•å¹³æ•°æ®ç»“æ„ç”¨äºCSV
            data = metrics.to_dict()
            flattened_data = {}
            
            # åŸºç¡€ä¿¡æ¯
            flattened_data.update({
                'request_id': data['request_id'],
                'trace_id': data['trace_id'],
                'success': data['success'],
                'error_message': data['error_message'],
                'timestamp': data['timestamp'],
                'send_time_iso': data['send_time_iso'],
            })
            
            # å®¢æˆ·ç«¯æŒ‡æ ‡
            for key, value in data['client_metrics'].items():
                flattened_data[f'client_{key}'] = value
            
            # æœåŠ¡ç«¯æŒ‡æ ‡
            for key, value in data['server_metrics'].items():
                flattened_data[f'server_{key}'] = value
            
            # Tokenä¿¡æ¯
            for key, value in data['tokens'].items():
                flattened_data[key] = value
            
            # è¯·æ±‚å‚æ•°
            for key, value in data['request_params'].items():
                flattened_data[f'param_{key}'] = value
            
            # å†…å®¹ä¿¡æ¯
            flattened_data['generated_text'] = data['content']['generated_text']
            flattened_data['text_length'] = data['content']['text_length']
            
            # ITLåˆ—è¡¨ (è½¬æ¢ä¸ºå­—ç¬¦ä¸²)
            itl_list_ms = data['detailed_data']['itl_list_ms']
            flattened_data['itl_list_ms'] = str(itl_list_ms) if itl_list_ms else "[]"
            flattened_data['itl_count'] = len(itl_list_ms) if itl_list_ms else 0
            
            # å†™å…¥CSV
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=flattened_data.keys())
                writer.writeheader()
                writer.writerow(flattened_data)
            
            print(f"âœ… è¯·æ±‚æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            return True
            
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ ¼å¼: {format}")
            return False
            
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
        return False


def save_multiple_requests_metrics(metrics_list: List[RequestMetrics], filename: str, format: str = "json") -> bool:
    """
    ä¿å­˜å¤šä¸ªè¯·æ±‚çš„æŒ‡æ ‡æ•°æ®åˆ°æ–‡ä»¶
    
    Args:
        metrics_list: RequestMetricså¯¹è±¡åˆ—è¡¨
        filename: æ–‡ä»¶å (ä¸å«æ‰©å±•å)
        format: æ–‡ä»¶æ ¼å¼ ("json" æˆ– "csv")
    
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    import json
    import csv
    from datetime import datetime
    
    if not metrics_list:
        print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return False
    
    try:
        if format.lower() == "json":
            # JSONæ ¼å¼ä¿å­˜
            filepath = f"{filename}.json"
            data = {
                "metadata": {
                    "total_requests": len(metrics_list),
                    "successful_requests": sum(1 for m in metrics_list if m.success),
                    "failed_requests": sum(1 for m in metrics_list if not m.success),
                    "export_time": datetime.now().isoformat(),
                    "format_version": "1.0"
                },
                "requests": [metrics.to_dict() for metrics in metrics_list]
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… {len(metrics_list)} ä¸ªè¯·æ±‚æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            return True
            
        elif format.lower() == "csv":
            # CSVæ ¼å¼ä¿å­˜
            filepath = f"{filename}.csv"
            
            if not metrics_list:
                print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
                return False
            
            # è·å–ç¬¬ä¸€ä¸ªè¯·æ±‚çš„æ•°æ®ç»“æ„ä½œä¸ºå‚è€ƒ
            first_data = metrics_list[0].to_dict()
            
            # å®šä¹‰CSVåˆ—
            fieldnames = [
                'request_id', 'trace_id', 'success', 'error_message', 
                'timestamp', 'send_time_iso',
                'client_e2e_latency_ms', 'client_ttft_ms', 'client_tpot_ms', 'client_itl_ms',
                'server_queue_time_ms', 'server_prefill_time_ms', 'server_decode_time_ms',
                'server_inference_time_ms', 'server_e2e_time_ms', 'server_ttft_ms',
                'prompt_tokens', 'completion_tokens',
                'param_model', 'param_temperature', 'param_top_p', 'param_max_tokens',
                'generated_text', 'text_length',
                'itl_list_ms', 'itl_count'
            ]
            
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for metrics in metrics_list:
                    data = metrics.to_dict()
                    
                    # å±•å¹³æ•°æ®
                    row = {
                        'request_id': data['request_id'],
                        'trace_id': data['trace_id'],
                        'success': data['success'],
                        'error_message': data['error_message'],
                        'timestamp': data['timestamp'],
                        'send_time_iso': data['send_time_iso'],
                        
                        'client_e2e_latency_ms': data['client_metrics']['e2e_latency_ms'],
                        'client_ttft_ms': data['client_metrics']['ttft_ms'],
                        'client_tpot_ms': data['client_metrics']['tpot_ms'],
                        'client_itl_ms': data['client_metrics']['itl_ms'],
                        
                        'server_queue_time_ms': data['server_metrics']['queue_time_ms'],
                        'server_prefill_time_ms': data['server_metrics']['prefill_time_ms'],
                        'server_decode_time_ms': data['server_metrics']['decode_time_ms'],
                        'server_inference_time_ms': data['server_metrics']['inference_time_ms'],
                        'server_e2e_time_ms': data['server_metrics']['e2e_time_ms'],
                        'server_ttft_ms': data['server_metrics']['ttft_ms'],
                        
                        
                        'prompt_tokens': data['tokens']['prompt_tokens'],
                        'completion_tokens': data['tokens']['completion_tokens'],
                        
                        'param_model': data['request_params']['model'],
                        'param_temperature': data['request_params']['temperature'],
                        'param_top_p': data['request_params']['top_p'],
                        'param_max_tokens': data['request_params']['max_tokens'],
                        
                        'generated_text': data['content']['generated_text'],
                        'text_length': data['content']['text_length'],
                        
                        'itl_list_ms': str(data['detailed_data']['itl_list_ms']),
                        'itl_count': len(data['detailed_data']['itl_list_ms']) if data['detailed_data']['itl_list_ms'] else 0
                    }
                    
                    writer.writerow(row)
            
            print(f"âœ… {len(metrics_list)} ä¸ªè¯·æ±‚æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            return True
            
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ ¼å¼: {format}")
            return False
            
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
        return False


def generate_timestamped_filename(prefix: str = "vllm_metrics") -> str:
    """
    ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    
    Args:
        prefix: æ–‡ä»¶åå‰ç¼€
        
    Returns:
        str: å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å (ä¸å«æ‰©å±•å)
    """
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{prefix}_{timestamp}"
