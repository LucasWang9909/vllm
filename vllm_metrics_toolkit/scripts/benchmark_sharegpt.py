#!/usr/bin/env python3
"""
ShareGPTåŸºå‡†æµ‹è¯•è„šæœ¬
ä½¿ç”¨ShareGPTæ•°æ®é›†çš„å‰1000ä¸ªå¯¹è¯è¿›è¡ŒvLLMæ€§èƒ½æµ‹è¯•

ç‰¹ç‚¹:
- æ”¯æŒè‡ªå®šä¹‰RPS (æ¯ç§’è¯·æ±‚æ•°)
- ä½¿ç”¨vllm_metrics_clientè¿›è¡Œå‡†ç¡®çš„æ€§èƒ½æŒ‡æ ‡æµ‹é‡
- è‡ªåŠ¨ä¿å­˜è¯¦ç»†çš„æµ‹è¯•ç»“æœ
- æ”¯æŒå¹¶å‘æµ‹è¯•å’Œé€Ÿç‡é™åˆ¶

ä½¿ç”¨æ–¹æ³•:
python benchmark_sharegpt.py --rps 10 --num_prompts 100
"""

import asyncio
import json
import time
import argparse
from typing import List, Dict, Any
from pathlib import Path
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from vllm_metrics_client import (
    VLLMMetricsClient, 
    RequestMetrics, 
    calculate_benchmark_summary,
    format_benchmark_summary,
    save_multiple_requests_metrics,
    generate_timestamped_filename
)


def load_sharegpt_prompts(dataset_path: str, num_prompts: int = 1000) -> List[str]:
    """
    ä»ShareGPTæ•°æ®é›†ä¸­æå–å‰Nä¸ªhumanæç¤º
    
    Args:
        dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        num_prompts: è¦æå–çš„æç¤ºæ•°é‡
    
    Returns:
        æç¤ºåˆ—è¡¨
    """
    print(f"Loading ShareGPT dataset from {dataset_path}...")
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    prompts = []
    for item in data[:num_prompts]:
        # æŸ¥æ‰¾conversationsä¸­çš„ç¬¬ä¸€ä¸ªhumanæ¶ˆæ¯
        for conv in item.get('conversations', []):
            if conv.get('from') == 'human':
                prompt = conv.get('value', '').strip()
                if prompt:  # ç¡®ä¿æç¤ºä¸ä¸ºç©º
                    prompts.append(prompt)
                break  # åªå–æ¯ä¸ªå¯¹è¯çš„ç¬¬ä¸€ä¸ªhumanæ¶ˆæ¯
    
    print(f"Extracted {len(prompts)} prompts from dataset")
    return prompts


class RateLimitedBenchmark:
    """æ”¯æŒé€Ÿç‡é™åˆ¶çš„åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, rps: float, vllm_url: str = "http://localhost:8000", 
                 jaeger_url: str = "http://localhost:16686"):
        self.rps = rps
        self.interval = 1.0 / rps if rps > 0 else 0
        self.client = VLLMMetricsClient(
            vllm_base_url=vllm_url,
            jaeger_base_url=jaeger_url
        )
        self.results: List[RequestMetrics] = []
    
    async def run_benchmark(self, prompts: List[str], 
                          temperature: float = 0.7, 
                          max_tokens: int = 150) -> List[RequestMetrics]:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            prompts: è¦æµ‹è¯•çš„æç¤ºåˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        
        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        print(f"\nğŸš€ Starting benchmark with {len(prompts)} prompts at {self.rps} RPS")
        print(f"   Temperature: {temperature}, Max tokens: {max_tokens}")
        print(f"   Estimated duration: {len(prompts) / self.rps:.1f} seconds\n")
        
        start_time = time.time()
        
        if self.rps <= 0:
            # æ— é€Ÿç‡é™åˆ¶ï¼Œå¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
            tasks = []
            for i, prompt in enumerate(prompts):
                task = self._send_single_request(prompt, i, temperature, max_tokens)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
            self.results = [r for r in results if isinstance(r, RequestMetrics)]
        else:
            # çœŸæ­£çš„å¼‚æ­¥RPSæ§åˆ¶ - æŒ‰é—´éš”å¯åŠ¨è¯·æ±‚ï¼Œä½†ä¸ç­‰å¾…å®Œæˆ
            tasks = []
            
            for i, prompt in enumerate(prompts):
                # åˆ›å»ºä¸€ä¸ªå»¶è¿Ÿå¯åŠ¨çš„ä»»åŠ¡
                delay = i * self.interval
                task = asyncio.create_task(
                    self._send_request_with_delay(prompt, i, delay, temperature, max_tokens)
                )
                tasks.append(task)
                
                # è¿›åº¦æ˜¾ç¤ºï¼ˆåŸºäºå¯åŠ¨çš„è¯·æ±‚æ•°ï¼‰
                if (i + 1) % 50 == 0 or i + 1 == len(prompts):
                    print(f"Scheduled: {i+1}/{len(prompts)} requests")
            
            print(f"All {len(prompts)} requests scheduled. Waiting for completion...")
            
            # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆï¼ŒåŒæ—¶æ˜¾ç¤ºè¿›åº¦
            results = await self._gather_with_progress(tasks, len(prompts))
            # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
            self.results = [r for r in results if isinstance(r, RequestMetrics)]
            
            # æ˜¾ç¤ºæœ€ç»ˆå®ŒæˆçŠ¶æ€
            successful = sum(1 for r in self.results if r.success)
            print(f"\nâœ… All requests completed: {len(self.results)} total ({successful} successful)")
        
        end_time = time.time()
        
        # æ”¶é›†æœåŠ¡ç«¯æŒ‡æ ‡
        print("\nğŸ“Š Collecting server metrics...")
        await self._collect_server_metrics()
        
        return self.results, start_time, end_time
    
    async def _gather_with_progress(self, tasks: List[asyncio.Task], total: int) -> List[RequestMetrics]:
        """ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ŒåŒæ—¶æ˜¾ç¤ºå®æ—¶è¿›åº¦"""
        completed = 0
        results = []
        
        # ä½¿ç”¨asyncio.as_completedæ¥è·å–å®æ—¶å®ŒæˆçŠ¶æ€
        for coro in asyncio.as_completed(tasks):
            try:
                result = await coro
                if isinstance(result, RequestMetrics):
                    results.append(result)
                completed += 1
                
                # æ¯å®Œæˆ50ä¸ªæˆ–è¾¾åˆ°é‡Œç¨‹ç¢‘æ—¶æ˜¾ç¤ºè¿›åº¦
                if completed % 50 == 0 or completed == total:
                    successful = sum(1 for r in results if r.success)
                    progress = completed / total * 100
                    print(f"Progress: {completed}/{total} ({progress:.1f}%) - "
                          f"Successful: {successful}")
                    
            except Exception as e:
                completed += 1
                print(f"Request failed: {e}")
        
        return results
    
    async def _send_request_with_delay(self, prompt: str, index: int, delay: float,
                                     temperature: float, max_tokens: int) -> RequestMetrics:
        """å»¶è¿Ÿå‘é€å•ä¸ªè¯·æ±‚ï¼ˆç”¨äºRPSæ§åˆ¶ï¼‰"""
        await asyncio.sleep(delay)
        return await self._send_single_request(prompt, index, temperature, max_tokens)
    
    async def _send_single_request(self, prompt: str, index: int, 
                                 temperature: float, max_tokens: int) -> RequestMetrics:
        """å‘é€å•ä¸ªè¯·æ±‚"""
        try:
            # å°†promptè½¬æ¢ä¸ºOpenAI chatæ ¼å¼
            messages = [{"role": "user", "content": prompt}]
            
            metrics = await self.client.send_request(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True  # ä½¿ç”¨æµå¼å“åº”ä»¥è·å¾—æ›´å‡†ç¡®çš„TTFTæŒ‡æ ‡
            )
            
            return metrics
            
        except Exception as e:
            print(f"Error in request {index}: {e}")
            # è¿”å›ä¸€ä¸ªå¤±è´¥çš„metricså¯¹è±¡
            failed_metrics = RequestMetrics(
                request_id=f"req_{index}",
                trace_id="failed",
                success=False,
                error_message=str(e)
            )
            return failed_metrics
    
    async def _collect_server_metrics(self):
        """æ‰¹é‡æ”¶é›†æœåŠ¡ç«¯æŒ‡æ ‡"""
        successful_results = [r for r in self.results if r.success]
        
        if not successful_results:
            print("No successful requests to collect server metrics for")
            return
        
        print(f"Collecting server metrics for {len(successful_results)} successful requests...")
        
        # å¹¶å‘æ”¶é›†æœåŠ¡ç«¯æŒ‡æ ‡
        tasks = []
        for metrics in successful_results:
            task = asyncio.create_task(
                self._collect_single_server_metrics(metrics)
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æŒ‡æ ‡æ”¶é›†å®Œæˆï¼Œè®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _collect_single_server_metrics(self, metrics: RequestMetrics):
        """æ”¶é›†å•ä¸ªè¯·æ±‚çš„æœåŠ¡ç«¯æŒ‡æ ‡"""
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼Œä½†åœ¨executorä¸­è¿è¡Œä»¥é¿å…é˜»å¡
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None, 
                self.client.enrich_with_server_metrics, 
                metrics, 
                20  # è¶…æ—¶æ—¶é—´
            )
        except Exception as e:
            print(f"Failed to collect server metrics for {metrics.request_id}: {e}")


def print_results_summary(results: List[RequestMetrics], start_time: float, end_time: float):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    summary = calculate_benchmark_summary(results, start_time, end_time)
    print("\n" + "="*80)
    print("ğŸ“ˆ BENCHMARK SUMMARY")
    print("="*80)
    print(format_benchmark_summary(summary))


def save_results(results: List[RequestMetrics], base_filename: str):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    print(f"\nğŸ’¾ Saving results...")
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆåŒ…å«å®Œæ•´æ•°æ®ï¼‰
    json_filename = f"{base_filename}.json"
    save_multiple_requests_metrics(results, json_filename, format="json")
    print(f"   âœ… Detailed results saved to: {json_filename}")
    
    # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä¾¿äºåˆ†æï¼‰
    csv_filename = f"{base_filename}.csv"
    save_multiple_requests_metrics(results, csv_filename, format="csv")
    print(f"   âœ… CSV results saved to: {csv_filename}")


async def main():
    parser = argparse.ArgumentParser(description="ShareGPTåŸºå‡†æµ‹è¯•")
    parser.add_argument("--rps", type=float, default=1.0, 
                       help="æ¯ç§’è¯·æ±‚æ•° (RPS). è®¾ä¸º0åˆ™æ— é™åˆ¶å¹¶å‘")
    parser.add_argument("--num_prompts", type=int, default=1000,
                       help="è¦æµ‹è¯•çš„æç¤ºæ•°é‡")
    parser.add_argument("--dataset_path", type=str, 
                       default="benchmark_datasets/ShareGPT_V3_unfiltered_cleaned_split.json",
                       help="ShareGPTæ•°æ®é›†è·¯å¾„")
    parser.add_argument("--vllm_url", type=str, default="http://localhost:8000",
                       help="vLLMæœåŠ¡URL")
    parser.add_argument("--jaeger_url", type=str, default="http://localhost:16686",
                       help="Jaeger UI URL")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="æ¸©åº¦å‚æ•°")
    parser.add_argument("--max_tokens", type=int, default=150,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--output_prefix", type=str, default="sharegpt_benchmark",
                       help="è¾“å‡ºæ–‡ä»¶å‰ç¼€")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    dataset_path = Path(args.dataset_path)
    if not dataset_path.exists():
        print(f"âŒ Dataset file not found: {dataset_path}")
        return
    
    try:
        # åŠ è½½æç¤º
        prompts = load_sharegpt_prompts(str(dataset_path), args.num_prompts)
        if not prompts:
            print("âŒ No prompts loaded from dataset")
            return
        
        # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
        benchmark = RateLimitedBenchmark(
            rps=args.rps,
            vllm_url=args.vllm_url,
            jaeger_url=args.jaeger_url
        )
        
        # è¿è¡Œæµ‹è¯•
        results, start_time, end_time = await benchmark.run_benchmark(
            prompts=prompts,
            temperature=args.temperature,
            max_tokens=args.max_tokens
        )
        
        if not results:
            print("âŒ No results obtained from benchmark")
            return
        
        # æ‰“å°æ‘˜è¦
        print_results_summary(results, start_time, end_time)
        
        # ä¿å­˜ç»“æœ
        timestamp_filename = generate_timestamped_filename(args.output_prefix)
        save_results(results, timestamp_filename)
        
        print(f"\nâœ… Benchmark completed successfully!")
        print(f"   ğŸ“Š Total requests: {len(results)}")
        print(f"   âœ… Successful: {sum(1 for r in results if r.success)}")
        print(f"   âŒ Failed: {sum(1 for r in results if not r.success)}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Benchmark interrupted by user")
    except Exception as e:
        print(f"âŒ Benchmark failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
