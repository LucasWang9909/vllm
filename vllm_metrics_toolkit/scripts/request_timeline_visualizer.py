#!/usr/bin/env python3
"""
è¯·æ±‚æ—¶é—´çº¿å¯è§†åŒ–å·¥å…·
åˆ›å»ºç”˜ç‰¹å›¾æ ·å¼çš„å¯è§†åŒ–ï¼Œæ˜¾ç¤ºæ¯ä¸ªè¯·æ±‚çš„è¯¦ç»†å¤„ç†é˜¶æ®µ:
- é˜Ÿåˆ—æ—¶é—´ (gen_ai.latency.time_in_queue)
- é¢„å¡«å……æ—¶é—´ (gen_ai.latency.time_in_model_prefill) 
- æ¯ä¸ªtokençš„ITL (Inter-token Latency)
- å½“å‰æ—¶é—´ç‚¹çš„å¹¶å‘è¯·æ±‚æ•°åˆ†æ
"""

import json
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.colors import ListedColormap
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
import argparse
from pathlib import Path


class RequestTimelineData:
    """å•ä¸ªè¯·æ±‚çš„æ—¶é—´çº¿æ•°æ®"""
    def __init__(self, request_data: Dict[str, Any]):
        self.request_id = request_data.get('request_id', 'unknown')
        self.start_time = request_data.get('timestamp', 0)
        self.success = request_data.get('success', False)
        
        # æœåŠ¡ç«¯æŒ‡æ ‡
        server_metrics = request_data.get('server_metrics', {})
        self.queue_time_ms = server_metrics.get('queue_time_ms', 0)
        self.prefill_time_ms = server_metrics.get('prefill_time_ms', 0)
        self.decode_time_ms = server_metrics.get('decode_time_ms', 0)
        
        
        # å®¢æˆ·ç«¯æŒ‡æ ‡
        client_metrics = request_data.get('client_metrics', {})
        self.client_ttft_ms = client_metrics.get('ttft_ms', 0)
        
        # Tokenä¿¡æ¯
        tokens = request_data.get('tokens', {})
        self.prompt_tokens = tokens.get('prompt_tokens', 0)
        self.completion_tokens = tokens.get('completion_tokens', 0)
        
        # ITLè¯¦ç»†æ•°æ®
        detailed_data = request_data.get('detailed_data', {})
        self.itl_list_ms = detailed_data.get('itl_list_ms', [])
        
        # è®¡ç®—å…³é”®æ—¶é—´ç‚¹
        self._calculate_timeline()
    
    def _calculate_timeline(self):
        """è®¡ç®—è¯·æ±‚çš„å„ä¸ªæ—¶é—´ç‚¹"""
        # æ‰€æœ‰æ—¶é—´éƒ½è½¬æ¢ä¸ºç›¸å¯¹äºstart_timeçš„ç§’æ•°
        self.queue_start = 0
        self.queue_end = self.queue_time_ms / 1000.0
        self.prefill_start = self.queue_end

        self.prefill_end = self.prefill_start + (self.prefill_time_ms / 1000.0)
        
        # Tokenç”Ÿæˆæ—¶é—´ç‚¹
        self.token_times = []
        current_time = self.prefill_end
        
        for i, itl_ms in enumerate(self.itl_list_ms):
            current_time += itl_ms / 1000.0
            self.token_times.append(current_time)
        
        # æ€»ç»“æŸæ—¶é—´
        self.end_time = self.token_times[-1] if self.token_times else self.prefill_end
        self.total_duration = self.end_time - self.queue_start


class ConcurrentRequestsAnalyzer:
    """å¹¶å‘è¯·æ±‚æ•°åˆ†æå™¨"""
    
    def __init__(self, timeline_data: List[RequestTimelineData]):
        self.timelines = timeline_data
        self.global_start_time = min(t.start_time for t in timeline_data) if timeline_data else 0
        
    def calculate_concurrent_requests(self, time_resolution: float = 0.1) -> Tuple[List[float], List[int]]:
        """
        è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„å¹¶å‘è¯·æ±‚æ•°
        
        Args:
            time_resolution: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆç§’ï¼‰
        
        Returns:
            (æ—¶é—´ç‚¹åˆ—è¡¨, å¹¶å‘æ•°åˆ—è¡¨)
        """
        if not self.timelines:
            return [], []
        
        # è®¡ç®—å…¨å±€æ—¶é—´èŒƒå›´
        max_end_time = max(t.start_time + t.total_duration for t in self.timelines)
        duration = max_end_time - self.global_start_time
        
        # ç”Ÿæˆæ—¶é—´ç‚¹
        time_points = np.arange(0, duration + time_resolution, time_resolution)
        concurrent_counts = []
        
        for t in time_points:
            absolute_time = self.global_start_time + t
            count = 0
            
            for timeline in self.timelines:
                request_start = timeline.start_time
                request_end = timeline.start_time + timeline.total_duration
                
                if request_start <= absolute_time <= request_end:
                    count += 1
            
            concurrent_counts.append(count)
        
        return time_points.tolist(), concurrent_counts


class RequestTimelineVisualizer:
    """è¯·æ±‚æ—¶é—´çº¿å¯è§†åŒ–å™¨"""
    
    def __init__(self, benchmark_data: Dict[str, Any]):
        self.data = benchmark_data
        self.timelines = []
        self._load_timelines()
        self.itl_stats = self._calculate_itl_statistics()
    
    def _load_timelines(self):
        """åŠ è½½æ‰€æœ‰è¯·æ±‚çš„æ—¶é—´çº¿æ•°æ®"""
        print("ğŸ“Š Loading request timelines...")
        
        for request in self.data.get('requests', []):
            if request.get('success', False):
                timeline = RequestTimelineData(request)
                if timeline.total_duration > 0:  # ç¡®ä¿æœ‰æœ‰æ•ˆçš„æ—¶é—´æ•°æ®
                    self.timelines.append(timeline)
        
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        self.timelines.sort(key=lambda x: x.start_time)
        
        print(f"âœ… Loaded {len(self.timelines)} valid request timelines")
    
    def _calculate_itl_statistics(self) -> Dict[str, float]:
        """è®¡ç®—æ‰€æœ‰ITLçš„ç»Ÿè®¡ä¿¡æ¯"""
        all_itls = []
        for timeline in self.timelines:
            # å°†ITLä»æ¯«ç§’è½¬æ¢ä¸ºç§’
            itls_seconds = [itl_ms / 1000.0 for itl_ms in timeline.itl_list_ms]
            all_itls.extend(itls_seconds)
        
        if not all_itls:
            return {'mean': 0, 'std': 0}
        
        mean_itl = np.mean(all_itls)
        std_itl = np.std(all_itls)
        
        print(f"ğŸ“Š ITL Statistics: Mean={mean_itl*1000:.1f}ms, Std={std_itl*1000:.1f}ms")
        
        return {
            'mean': mean_itl,
            'std': std_itl,
            'threshold_1': mean_itl,
            'threshold_2': mean_itl + std_itl,
            'threshold_3': mean_itl + 2 * std_itl
        }
    
    def _get_itl_color(self, itl_duration_seconds: float) -> str:
        """æ ¹æ®ITLé•¿åº¦è¿”å›å¯¹åº”çš„é¢œè‰²"""
        if itl_duration_seconds <= self.itl_stats['threshold_1']:
            return '#2ECC71'  # ç»¿è‰² - æ­£å¸¸ITL
        elif itl_duration_seconds <= self.itl_stats['threshold_2']:
            return '#F39C12'  # é»„è‰² - ç¨é•¿ITL
        elif itl_duration_seconds <= self.itl_stats['threshold_3']:
            return '#E67E22'  # æ©™è‰² - é•¿ITL
        else:
            return '#E74C3C'  # çº¢è‰² - å¼‚å¸¸é•¿ITL
    
    def create_gantt_chart(self, max_requests: int = 50, figsize: Tuple[int, int] = (25, 12)):
        """
        åˆ›å»ºç”˜ç‰¹å›¾æ˜¾ç¤ºè¯·æ±‚æ—¶é—´çº¿
        
        Args:
            max_requests: æœ€å¤§æ˜¾ç¤ºè¯·æ±‚æ•°ï¼ˆé¿å…å›¾è¡¨è¿‡äºå¤æ‚ï¼‰
            figsize: å›¾è¡¨å¤§å°
        """
        if not self.timelines:
            print("âŒ No timeline data available")
            return None
        
        # é™åˆ¶æ˜¾ç¤ºçš„è¯·æ±‚æ•°é‡
        display_timelines = self.timelines[:max_requests]
        
        # è®¡ç®—ç›¸å¯¹æ—¶é—´åŸºå‡†
        global_start = min(t.start_time for t in display_timelines)
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, height_ratios=[3, 1])
        
        # é¢œè‰²é…ç½®
        colors = {
            'queue': '#FF6B6B',      # çº¢è‰² - é˜Ÿåˆ—ç­‰å¾…
            'prefill': '#4ECDC4',    # é’è‰² - é¢„å¡«å……
            'token': '#45B7D1',      # è“è‰² - Tokenç”Ÿæˆ
            'itl': '#96CEB4'         # ç»¿è‰² - ITLé—´éš”
        }
        
        # ç»˜åˆ¶ç”˜ç‰¹å›¾
        for i, timeline in enumerate(display_timelines):
            y_pos = i
            start_offset = timeline.start_time - global_start
            
            # é˜Ÿåˆ—æ—¶é—´
            if timeline.queue_time_ms > 0:
                queue_rect = patches.Rectangle(
                    (start_offset + timeline.queue_start, y_pos - 0.4),
                    timeline.queue_end - timeline.queue_start, 0.8,
                    facecolor=colors['queue'], alpha=0.8, label='Queue' if i == 0 else ""
                )
                ax1.add_patch(queue_rect)
            
            # é¢„å¡«å……æ—¶é—´
            if timeline.prefill_time_ms > 0:
                prefill_rect = patches.Rectangle(
                    (start_offset + timeline.prefill_start, y_pos - 0.4),
                    timeline.prefill_end - timeline.prefill_start, 0.8,
                    facecolor=colors['prefill'], alpha=0.8, label='Prefill' if i == 0 else ""
                )
                ax1.add_patch(prefill_rect)
            
            # Tokenç”Ÿæˆï¼ˆæ¯ä¸ªITLä½œä¸ºå•ç‹¬çš„æ®µï¼Œæ ¹æ®é•¿åº¦ç”¨ä¸åŒé¢œè‰²ï¼‰
            prev_time = timeline.prefill_end
            for token_time in timeline.token_times:
                itl_duration = token_time - prev_time
                itl_color = self._get_itl_color(itl_duration)
                
                token_rect = patches.Rectangle(
                    (start_offset + prev_time, y_pos - 0.3),
                    itl_duration, 0.6,
                    facecolor=itl_color,
                    edgecolor=(0, 0, 0, 0.5),
                    linewidth=0.2,
                    alpha=0.85
                )
                ax1.add_patch(token_rect)
                prev_time = token_time
            # æœ€åä¸€ä¸ª token æ®µå·²é€šè¿‡è¾¹æ¡†ä½“ç°åˆ†éš”ï¼Œæ— éœ€é¢å¤–çº¿æ¡
        
        # è®¾ç½®ç”˜ç‰¹å›¾æ ·å¼
        ax1.set_xlim(0, max(t.start_time - global_start + t.total_duration for t in display_timelines))
        ax1.set_ylim(-0.5, len(display_timelines) - 0.5)
        ax1.set_xlabel('Time (seconds)', fontsize=12)
        ax1.set_ylabel('Request Index', fontsize=12)
        ax1.set_title(f'Request Timeline Visualization (First {len(display_timelines)} Requests)', 
                     fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ ITLé¢œè‰²åˆ†çº§å›¾ä¾‹
        itl_legend_elements = [
            patches.Patch(color='#FF6B6B', label='Queue'),
            patches.Patch(color='#4ECDC4', label='Encode+Prefill'),
            patches.Patch(color='#2ECC71', label=f'Normal ITL (<{self.itl_stats["threshold_1"]*1000:.0f}ms)'),
            patches.Patch(color='#F39C12', label=f'Long ITL ({self.itl_stats["threshold_1"]*1000:.0f}-{self.itl_stats["threshold_2"]*1000:.0f}ms)'),
            patches.Patch(color='#E67E22', label=f'Longer ITL ({self.itl_stats["threshold_2"]*1000:.0f}-{self.itl_stats["threshold_3"]*1000:.0f}ms)'),
            patches.Patch(color='#E74C3C', label=f'Abnormal ITL (>{self.itl_stats["threshold_3"]*1000:.0f}ms)')
        ]
        ax1.legend(handles=itl_legend_elements, loc='upper right', bbox_to_anchor=(1.0, 1.0))
        
        # åè½¬Yè½´ï¼ˆæœ€æ–°è¯·æ±‚åœ¨ä¸Šæ–¹ï¼‰
        ax1.invert_yaxis()
        
        # æ·»åŠ å¹¶å‘è¯·æ±‚æ•°åˆ†æ
        analyzer = ConcurrentRequestsAnalyzer(self.timelines)
        time_points, concurrent_counts = analyzer.calculate_concurrent_requests()
        
        ax2.plot(time_points, concurrent_counts, linewidth=2, color='#E74C3C')
        ax2.fill_between(time_points, concurrent_counts, alpha=0.3, color='#E74C3C')
        ax2.set_xlabel('Time (seconds)', fontsize=12)
        ax2.set_ylabel('Concurrent Requests', fontsize=12)
        ax2.set_title('Number of Concurrent Requests Over Time', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        max_concurrent = max(concurrent_counts) if concurrent_counts else 0
        avg_concurrent = np.mean(concurrent_counts) if concurrent_counts else 0
        
        stats_text = f'Max Concurrent: {max_concurrent}\nAvg Concurrent: {avg_concurrent:.1f}'
        ax2.text(0.02, 0.98, stats_text, transform=ax2.transAxes, 
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        return fig
    
    def create_detailed_request_view(self, request_indices: List[int] = None, figsize: Tuple[int, int] = (15, 8)):
        """
        åˆ›å»ºè¯¦ç»†çš„å•ä¸ªæˆ–å¤šä¸ªè¯·æ±‚è§†å›¾ï¼Œæ˜¾ç¤ºæ¯ä¸ªITL
        
        Args:
            request_indices: è¦æ˜¾ç¤ºçš„è¯·æ±‚ç´¢å¼•åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå‰å‡ ä¸ª
            figsize: å›¾è¡¨å¤§å°
        """
        if not self.timelines:
            print("âŒ No timeline data available")
            return None
        
        if request_indices is None:
            request_indices = list(range(min(10, len(self.timelines))))
        
        selected_timelines = [self.timelines[i] for i in request_indices if i < len(self.timelines)]
        
        if not selected_timelines:
            print("âŒ No valid request indices")
            return None
        
        # åˆ›å»ºå›¾è¡¨
        fig, ax = plt.subplots(figsize=figsize)
        
        global_start = min(t.start_time for t in selected_timelines)
        
        colors = {
            'queue': '#FF6B6B',
            'prefill': '#4ECDC4', 
            'encoder': '#9B59B6',
            'token': '#45B7D1'
        }
        
        for i, timeline in enumerate(selected_timelines):
            y_pos = i * 2  # å¢åŠ é—´è·ä»¥æ˜¾ç¤ºæ›´å¤šç»†èŠ‚
            start_offset = timeline.start_time - global_start
            
            # é˜Ÿåˆ—æ—¶é—´
            if timeline.queue_time_ms > 0:
                ax.barh(y_pos, timeline.queue_end - timeline.queue_start, 
                       left=start_offset + timeline.queue_start, height=0.6,
                       color=colors['queue'], alpha=0.8, label='Queue' if i == 0 else "")
            
            # é¢„å¡«å……æ—¶é—´
            if timeline.prefill_time_ms > 0:
                if timeline.encoder_start is not None and timeline.encoder_end is not None:
                    ax.barh(y_pos, timeline.encoder_end - timeline.encoder_start,
                           left=start_offset + timeline.encoder_start, height=0.6,
                           color=colors['encoder'], alpha=0.8, label='Encoder' if i == 0 else "")
                    if timeline.true_prefill_start is not None and timeline.true_prefill_end is not None:
                        ax.barh(y_pos, timeline.true_prefill_end - timeline.true_prefill_start,
                               left=start_offset + timeline.true_prefill_start, height=0.6,
                               color=colors['prefill'], alpha=0.8, label='Prefill' if i == 0 else "")
                else:
                    ax.barh(y_pos, timeline.prefill_end - timeline.prefill_start,
                           left=start_offset + timeline.prefill_start, height=0.6,
                           color=colors['prefill'], alpha=0.8, label='Prefill' if i == 0 else "")
            
            # æ¯ä¸ªTokençš„ITLï¼ˆè¯¦ç»†æ˜¾ç¤ºï¼Œæ ¹æ®é•¿åº¦ç”¨ä¸åŒé¢œè‰²ï¼‰
            prev_time = timeline.prefill_end
            for token_time in timeline.token_times:
                itl_duration = token_time - prev_time
                itl_color = self._get_itl_color(itl_duration)
                
                ax.barh(y_pos, itl_duration, left=start_offset + prev_time, height=0.4,
                       color=itl_color, edgecolor=(0, 0, 0, 0.5), linewidth=0.2, alpha=0.85)
                
                # åœ¨æ¯ä¸ªtokenæ®µä¸Šæ ‡æ³¨ITLæ—¶é—´ï¼ˆæ™ºèƒ½é˜ˆå€¼ï¼‰
                # å¦‚æœITLè¶…è¿‡å¹³å‡å€¼ï¼Œå°±æ˜¾ç¤ºæ—¶é—´æ ‡æ³¨
                if itl_duration > self.itl_stats['threshold_1']:
                    ax.text(start_offset + prev_time + itl_duration/2, y_pos, 
                           f'{itl_duration*1000:.0f}ms', 
                           ha='center', va='center', fontsize=8, 
                           color='white', weight='bold')
                
                prev_time = token_time
            # æœ€åä¸€ä¸ª token æ®µå·²é€šè¿‡è¾¹æ¡†ä½“ç°åˆ†éš”ï¼Œæ— éœ€é¢å¤–çº¿æ¡
            
            # æ·»åŠ è¯·æ±‚IDæ ‡ç­¾
            ax.text(-0.5, y_pos, f'Req {request_indices[i]}\n{timeline.request_id[:8]}...', 
                   ha='right', va='center', fontsize=9)
        
        ax.set_xlabel('Time (seconds)', fontsize=12)
        ax.set_ylabel('Requests', fontsize=12)
        ax.set_title('Detailed Request Timeline with Individual Token Latencies', 
                    fontsize=14, fontweight='bold')
        
        # æ·»åŠ ITLé¢œè‰²åˆ†çº§å›¾ä¾‹
        itl_legend_elements = [
            patches.Patch(color='#FF6B6B', label='Queue'),
            patches.Patch(color='#4ECDC4', label='Encode+Prefill'),
            patches.Patch(color='#2ECC71', label=f'Normal ITL (<{self.itl_stats["threshold_1"]*1000:.0f}ms)'),
            patches.Patch(color='#F39C12', label=f'Long ITL ({self.itl_stats["threshold_1"]*1000:.0f}-{self.itl_stats["threshold_2"]*1000:.0f}ms)'),
            patches.Patch(color='#E67E22', label=f'Longer ITL ({self.itl_stats["threshold_2"]*1000:.0f}-{self.itl_stats["threshold_3"]*1000:.0f}ms)'),
            patches.Patch(color='#E74C3C', label=f'Abnormal ITL (>{self.itl_stats["threshold_3"]*1000:.0f}ms)')
        ]
        ax.legend(handles=itl_legend_elements, loc='upper right', bbox_to_anchor=(1.0, 1.0))
        ax.grid(True, alpha=0.3)
        
        # è®¾ç½®Yè½´æ ‡ç­¾
        ax.set_yticks([i * 2 for i in range(len(selected_timelines))])
        ax.set_yticklabels([f'Request {idx}' for idx in request_indices[:len(selected_timelines)]])
        
        plt.tight_layout()
        return fig
    
    def generate_statistics_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if not self.timelines:
            return {}
        
        # åŸºæœ¬ç»Ÿè®¡
        total_requests = len(self.timelines)
        
        # æ—¶é—´ç»Ÿè®¡
        queue_times = [t.queue_time_ms for t in self.timelines if t.queue_time_ms > 0]
        prefill_times = [t.prefill_time_ms for t in self.timelines if t.prefill_time_ms > 0]
        total_durations = [t.total_duration for t in self.timelines]
        
        # å¹¶å‘åˆ†æ
        analyzer = ConcurrentRequestsAnalyzer(self.timelines)
        _, concurrent_counts = analyzer.calculate_concurrent_requests()
        
        # Tokenç»Ÿè®¡
        completion_tokens = [t.completion_tokens for t in self.timelines]
        itl_counts = [len(t.itl_list_ms) for t in self.timelines]
        
        return {
            'total_requests': total_requests,
            'queue_time_stats': {
                'mean': np.mean(queue_times) if queue_times else 0,
                'median': np.median(queue_times) if queue_times else 0,
                'max': max(queue_times) if queue_times else 0,
                'min': min(queue_times) if queue_times else 0
            },
            'prefill_time_stats': {
                'mean': np.mean(prefill_times) if prefill_times else 0,
                'median': np.median(prefill_times) if prefill_times else 0,
                'max': max(prefill_times) if prefill_times else 0,
                'min': min(prefill_times) if prefill_times else 0
            },
            'duration_stats': {
                'mean': np.mean(total_durations),
                'median': np.median(total_durations),
                'max': max(total_durations),
                'min': min(total_durations)
            },
            'concurrency_stats': {
                'max_concurrent': max(concurrent_counts) if concurrent_counts else 0,
                'avg_concurrent': np.mean(concurrent_counts) if concurrent_counts else 0,
                'min_concurrent': min(concurrent_counts) if concurrent_counts else 0
            },
            'token_stats': {
                'avg_completion_tokens': np.mean(completion_tokens),
                'avg_itl_count': np.mean(itl_counts)
            }
        }


def main():
    parser = argparse.ArgumentParser(description="Visualize request timelines and concurrent processing")
    parser.add_argument("input_file", help="Path to benchmark JSON file")
    parser.add_argument("--max-requests", type=int, default=50, 
                       help="Maximum number of requests to show in gantt chart")
    parser.add_argument("--detailed-requests", nargs='+', type=int, 
                       help="Specific request indices for detailed view")
    parser.add_argument("--output-prefix", default="timeline", 
                       help="Output file prefix")
    parser.add_argument("--no-show", action="store_true", 
                       help="Don't display plots")
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ Loading data from: {args.input_file}")
    try:
        with open(args.input_file, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ Error loading file: {e}")
        return
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = RequestTimelineVisualizer(data)
    
    if not visualizer.timelines:
        print("âŒ No valid timeline data found")
        return
    
    # ç”Ÿæˆç”˜ç‰¹å›¾
    print("ğŸ“Š Creating Gantt chart...")
    gantt_fig = visualizer.create_gantt_chart(max_requests=args.max_requests)
    if gantt_fig:
        gantt_output = f"{args.output_prefix}_gantt.png"
        gantt_fig.savefig(gantt_output, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ Gantt chart saved to: {gantt_output}")
        
        if not args.no_show:
            plt.show()
    
    # ç”Ÿæˆè¯¦ç»†è§†å›¾
    if args.detailed_requests:
        print("ğŸ“Š Creating detailed request view...")
        detailed_fig = visualizer.create_detailed_request_view(args.detailed_requests)
        if detailed_fig:
            detailed_output = f"{args.output_prefix}_detailed.png"
            detailed_fig.savefig(detailed_output, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ Detailed view saved to: {detailed_output}")
            
            if not args.no_show:
                plt.show()
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = visualizer.generate_statistics_report()
    print(f"\nğŸ“ˆ Statistics Report:")
    print(f"   Total Requests: {stats.get('total_requests', 0)}")
    print(f"   Max Concurrent: {stats.get('concurrency_stats', {}).get('max_concurrent', 0)}")
    print(f"   Avg Concurrent: {stats.get('concurrency_stats', {}).get('avg_concurrent', 0):.1f}")
    print(f"   Avg Queue Time: {stats.get('queue_time_stats', {}).get('mean', 0):.1f}ms")
    print(f"   Avg Prefill Time: {stats.get('prefill_time_stats', {}).get('mean', 0):.1f}ms")
    print(f"   Avg Total Duration: {stats.get('duration_stats', {}).get('mean', 0):.1f}s")


if __name__ == "__main__":
    main()
