#!/usr/bin/env python3
"""
å¿«é€Ÿè¿è¡ŒShareGPTåŸºå‡†æµ‹è¯•çš„ç¤ºä¾‹è„šæœ¬
"""

import asyncio
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from benchmark_sharegpt import main, RateLimitedBenchmark, load_sharegpt_prompts
from vllm_metrics_client import generate_timestamped_filename


async def quick_test():
    """å¿«é€Ÿæµ‹è¯•ç¤ºä¾‹ - ä½¿ç”¨å‰10ä¸ªæç¤ºè¿›è¡Œæµ‹è¯•"""
    
    print("ğŸ”¬ Quick Test - Testing with first 10 prompts")
    print("="*50)
    
    # åŠ è½½å°‘é‡æç¤ºç”¨äºå¿«é€Ÿæµ‹è¯•
    dataset_path = "benchmark_datasets/ShareGPT_V3_unfiltered_cleaned_split.json"
    prompts = load_sharegpt_prompts(dataset_path, num_prompts=10)
    
    if not prompts:
        print("âŒ No prompts loaded")
        return
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = RateLimitedBenchmark(rps=2.0)  # 2 RPS for quick test
    
    # è¿è¡Œæµ‹è¯•
    results, start_time, end_time = await benchmark.run_benchmark(
        prompts=prompts,
        temperature=0.7,
        max_tokens=50  # è¾ƒå°‘çš„tokensç”¨äºå¿«é€Ÿæµ‹è¯•
    )
    
    # æ˜¾ç¤ºç»“æœ
    from vllm_metrics_client import calculate_benchmark_summary, format_benchmark_summary
    summary = calculate_benchmark_summary(results, start_time, end_time)
    print("\n" + format_benchmark_summary(summary))
    
    print(f"\nâœ… Quick test completed!")
    print(f"   ğŸ“Š Total requests: {len(results)}")
    print(f"   âœ… Successful: {sum(1 for r in results if r.success)}")


async def full_benchmark():
    """å®Œæ•´çš„1000ä¸ªæç¤ºåŸºå‡†æµ‹è¯•"""
    
    print("ğŸš€ Full Benchmark - 1000 prompts at 1 RPS")
    print("="*50)
    print("âš ï¸  This will take approximately 16-17 minutes to complete")
    
    response = input("Continue? (y/N): ")
    if response.lower() != 'y':
        print("Benchmark cancelled")
        return
    
    # åŠ è½½1000ä¸ªæç¤º
    dataset_path = "benchmark_datasets/ShareGPT_V3_unfiltered_cleaned_split.json"
    prompts = load_sharegpt_prompts(dataset_path, num_prompts=1000)
    
    if not prompts:
        print("âŒ No prompts loaded")
        return
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = RateLimitedBenchmark(rps=1.0)  # 1 RPS as requested
    
    # è¿è¡Œæµ‹è¯•
    results, start_time, end_time = await benchmark.run_benchmark(
        prompts=prompts,
        temperature=0.7,
        max_tokens=150
    )
    
    # æ˜¾ç¤ºç»“æœ
    from vllm_metrics_client import (
        calculate_benchmark_summary, 
        format_benchmark_summary,
        save_multiple_requests_metrics,
        generate_timestamped_filename
    )
    
    summary = calculate_benchmark_summary(results, start_time, end_time)
    print("\n" + format_benchmark_summary(summary))
    
    # ä¿å­˜ç»“æœ
    filename = generate_timestamped_filename("sharegpt_1000_rps1")
    save_multiple_requests_metrics(results, f"{filename}.json", format="json")
    save_multiple_requests_metrics(results, f"{filename}.csv", format="csv")
    
    print(f"\nâœ… Full benchmark completed!")
    print(f"   ğŸ“Š Total requests: {len(results)}")
    print(f"   âœ… Successful: {sum(1 for r in results if r.success)}")
    print(f"   ğŸ’¾ Results saved to: {filename}.json and {filename}.csv")


async def main():
    print("ShareGPT Benchmark Test Options")
    print("="*40)
    print("1. Quick test (10 prompts, 2 RPS)")
    print("2. Full benchmark (1000 prompts, 1 RPS)")
    print("3. Exit")
    
    choice = input("\nSelect option (1-3): ")
    
    if choice == "1":
        await quick_test()
    elif choice == "2":
        await full_benchmark()
    elif choice == "3":
        print("Goodbye!")
    else:
        print("Invalid choice")


if __name__ == "__main__":
    asyncio.run(main())
