# vLLM æ€§èƒ½æŒ‡æ ‡ç›‘æ§å·¥å…·åŒ…

ä¸€ä¸ªç”¨äºå‡†ç¡®æµ‹é‡ vLLM å•è¯·æ±‚æ€§èƒ½æŒ‡æ ‡çš„ Python å·¥å…·åŒ…ï¼Œæ”¯æŒå¹¶å‘åœºæ™¯ä¸‹çš„ç²¾ç¡®æŒ‡æ ‡è·å–ã€‚

## âœ¨ ç‰¹æ€§

- **ğŸ¯ ç²¾ç¡®æŒ‡æ ‡**: åŸºäº W3C Trace Context æ ‡å‡†ï¼Œç¡®ä¿æ¯ä¸ªè¯·æ±‚çš„æŒ‡æ ‡å‡†ç¡®å¯¹åº”
- **ğŸ“Š å…¨é¢ç›‘æ§**: åŒ…å«æ‰€æœ‰å…³é”®æ€§èƒ½æŒ‡æ ‡
  - **TTFT** (Time to First Token): é¦–æ¬¡tokenç”Ÿæˆæ—¶é—´
  - **TPOT** (Time per Output Token): æ¯tokenå¹³å‡ç”Ÿæˆæ—¶é—´  
  - **ITL** (Inter-token Latency): tokené—´å¹³å‡å»¶è¿Ÿ
  - **é˜Ÿåˆ—ç­‰å¾…æ—¶é—´**: è¯·æ±‚åœ¨é˜Ÿåˆ—ä¸­çš„ç­‰å¾…æ—¶é—´
  - **é¢„å¡«å……æ—¶é—´**: promptå¤„ç†æ—¶é—´
  - **è§£ç æ—¶é—´**: tokenç”Ÿæˆæ—¶é—´
  - **ç«¯åˆ°ç«¯å»¶è¿Ÿ**: å®Œæ•´è¯·æ±‚å“åº”æ—¶é—´
- **ğŸš€ å¹¶å‘æ”¯æŒ**: æ”¯æŒå¤šä¸ªå¹¶å‘è¯·æ±‚çš„ç‹¬ç«‹æŒ‡æ ‡æµ‹é‡
- **ğŸ’¾ æ•°æ®ä¿å­˜**: æ”¯æŒå°†è¯·æ±‚æŒ‡æ ‡ä¿å­˜ä¸ºJSON/CSVæ ¼å¼ï¼Œä¾¿äºåç»­åˆ†æ
- **ğŸ“ˆ ç”Ÿäº§å°±ç»ª**: å¯ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½ç›‘æ§

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### vLLM æœåŠ¡ç«¯é…ç½®

å¯åŠ¨ vLLM æ—¶éœ€è¦å¯ç”¨ OpenTelemetry è¿½è¸ªï¼š

```bash
# å¯åŠ¨æ”¯æŒ OpenTelemetry çš„ vLLM æœåŠ¡
vllm serve Qwen/Qwen2.5-Omni-7B \
  --otlp-traces-endpoint http://localhost:4317 \
  --collect-detailed-traces all \
  --port 8000
```

### Jaeger è¿½è¸ªæœåŠ¡

æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨ Docker å®¹å™¨æˆ–åŸç”Ÿå®‰è£…çš„æ–¹å¼è¿è¡Œ Jaegerï¼š

#### é€‰é¡¹1: Docker æ–¹å¼ (æ¨è)
```bash
# ä½¿ç”¨ Docker å¯åŠ¨ä¸€æ¬¡æ€§çš„Jaeger
docker run --rm --name jaeger \
  -p 16686:16686 -p 14250:14250 -p 4317:4317 -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

#### é€‰é¡¹2: åŸç”Ÿå®‰è£… (æ— å®¹å™¨)
å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨ Dockerï¼Œå¯ä»¥ä¸‹è½½å¹¶ç›´æ¥è¿è¡Œ Jaegerï¼š

```bash
# è¿è¡Œé¢„é…ç½®çš„å¯åŠ¨è„šæœ¬
cd /home/ubuntu/vllm/vllm_metrics_toolkit/jaeger
./start_jaeger.sh
```

æˆ–è€…æ‰‹åŠ¨å®‰è£…ï¼š
```bash
# ä¸‹è½½ Jaeger
wget https://github.com/jaegertracing/jaeger/releases/download/v1.52.0/jaeger-1.52.0-linux-amd64.tar.gz
tar -xzf jaeger-1.52.0-linux-amd64.tar.gz
cd jaeger-1.52.0-linux-amd64

# å¯åŠ¨ Jaeger All-in-One
./jaeger-all-in-one \
  --collector.otlp.enabled=true \
  --collector.otlp.grpc.host-port=0.0.0.0:4317 \
  --collector.otlp.http.host-port=0.0.0.0:4318 \
  --query.http-server.host-port=0.0.0.0:16686 \
  --admin.http.host-port=0.0.0.0:14269 \
  --log-level=info
```

ğŸ“– **è¯¦ç»†å®‰è£…æŒ‡å—**: å‚è§ [`docs/NATIVE_JAEGER_INSTALL.md`](./NATIVE_JAEGER_INSTALL.md)

## ğŸ“¦ å®‰è£…

1. å…‹éš†æˆ–ä¸‹è½½æ­¤å·¥å…·åŒ…
2. å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
import asyncio
from vllm_metrics_client import VLLMMetricsClient, format_metrics

async def basic_example():
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = VLLMMetricsClient(
        vllm_base_url="http://localhost:8000",      # vLLM æœåŠ¡åœ°å€
        jaeger_base_url="http://localhost:16686"    # Jaeger UI åœ°å€
    )
    
    # å‘é€è¯·æ±‚
    metrics = await client.send_request(
        messages=[{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],
        temperature=0.7,
        max_tokens=100
    )
    
    # è·å–æœåŠ¡ç«¯æŒ‡æ ‡
    if metrics.success:
        client.enrich_with_server_metrics(metrics, timeout=15)
    
    # æ˜¾ç¤ºç»“æœ
    print(format_metrics(metrics))

# è¿è¡Œ
asyncio.run(basic_example())
```

### æ•°æ®ä¿å­˜åŠŸèƒ½

```python
from vllm_metrics_client import (
    save_request_metrics, 
    save_multiple_requests_metrics,
    generate_timestamped_filename
)

# ä¿å­˜å•ä¸ªè¯·æ±‚æ•°æ®
filename = generate_timestamped_filename("my_test")
save_request_metrics(metrics, filename, format="json")  # ä¿å­˜ä¸ºJSON
save_request_metrics(metrics, filename, format="csv")   # ä¿å­˜ä¸ºCSV

# ä¿å­˜å¤šä¸ªè¯·æ±‚æ•°æ®
save_multiple_requests_metrics(results_list, filename, format="json")
```

**ä¿å­˜çš„æ•°æ®åŒ…å«:**
- **å‘é€æ—¶é—´**: è¯·æ±‚å‘é€çš„æ—¶é—´æˆ³
- **é˜Ÿåˆ—æ—¶é—´**: æœåŠ¡ç«¯é˜Ÿåˆ—ç­‰å¾…æ—¶é—´  
- **TTFT**: é¦–æ¬¡tokenç”Ÿæˆæ—¶é—´
- **TPOT**: æ¯tokenå¹³å‡ç”Ÿæˆæ—¶é—´
- **ITLåˆ—è¡¨**: å®Œæ•´çš„tokené—´å»¶è¿Ÿæ•°æ®
- **æ‰€æœ‰å…¶ä»–æŒ‡æ ‡**: E2Eå»¶è¿Ÿã€tokenç»Ÿè®¡ã€è¯·æ±‚å‚æ•°ç­‰

### å¹¶å‘è¯·æ±‚æµ‹è¯•

```python
async def concurrent_test():
    client = VLLMMetricsClient()
    
    # åˆ›å»ºå¤šä¸ªå¹¶å‘è¯·æ±‚
    tasks = []
    for i in range(3):
        task = client.send_request(
            messages=[{"role": "user", "content": f"è¯·æ±‚{i+1}: è§£é‡ŠAI"}],
            temperature=0.5 + i*0.2,
            max_tokens=50 + i*20
        )
        tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œ
    results = await asyncio.gather(*tasks)
    
    # è·å–æœåŠ¡ç«¯æŒ‡æ ‡
    for metrics in results:
        if metrics.success:
            client.enrich_with_server_metrics(metrics)
    
    # æ˜¾ç¤ºå¯¹æ¯”
    for i, metrics in enumerate(results):
        print(f"\nè¯·æ±‚{i+1}æŒ‡æ ‡:")
        print(format_metrics(metrics, show_details=False))

# è¿è¡Œ
asyncio.run(concurrent_test())
```

### åŸºå‡†æµ‹è¯•æ±‡æ€»ç»Ÿè®¡

```python
from vllm_metrics_client import calculate_benchmark_summary, format_benchmark_summary
import time

async def benchmark_test():
    client = VLLMMetricsClient()
    
    # å‘é€å¤šä¸ªæµ‹è¯•è¯·æ±‚
    start_time = time.time()
    results = []
    
    for i in range(10):
        metrics = await client.send_request(
            messages=[{"role": "user", "content": f"æµ‹è¯•è¯·æ±‚{i+1}"}],
            temperature=0.7,
            max_tokens=50
        )
        results.append(metrics)
    
    end_time = time.time()
    
    # è·å–æœåŠ¡ç«¯æŒ‡æ ‡
    for metrics in results:
        if metrics.success:
            client.enrich_with_server_metrics(metrics)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = calculate_benchmark_summary(results, start_time, end_time)
    print(format_benchmark_summary(summary))
```

## ğŸ“Š æŒ‡æ ‡è¯´æ˜

### å®¢æˆ·ç«¯æŒ‡æ ‡
- **client_e2e_latency**: å®¢æˆ·ç«¯æµ‹é‡çš„ç«¯åˆ°ç«¯å»¶è¿Ÿ (ms)
- **client_ttft**: å®¢æˆ·ç«¯æµ‹é‡çš„é¦–æ¬¡tokenæ—¶é—´ (ms)
- **client_tpot**: å®¢æˆ·ç«¯æµ‹é‡çš„æ¯tokenå¹³å‡æ—¶é—´ (ms)
- **client_itl**: å®¢æˆ·ç«¯æµ‹é‡çš„tokené—´å¹³å‡å»¶è¿Ÿ (ms)

### æœåŠ¡ç«¯æŒ‡æ ‡ (ä» OpenTelemetry è·å–)
- **server_queue_time**: è¯·æ±‚åœ¨é˜Ÿåˆ—ä¸­çš„ç­‰å¾…æ—¶é—´ (ms)
- **server_prefill_time**: prompté¢„å¤„ç†æ—¶é—´ (ms)
- **server_decode_time**: tokenç”Ÿæˆæ—¶é—´ (ms)
- **server_inference_time**: æ€»æ¨ç†æ—¶é—´ (ms)
- **server_e2e_time**: æœåŠ¡ç«¯ç«¯åˆ°ç«¯æ—¶é—´ (ms)
- **server_ttft**: æœåŠ¡ç«¯é¦–æ¬¡tokenæ—¶é—´ (ms)

### Token ç»Ÿè®¡
- **prompt_tokens**: è¾“å…¥tokenæ•°é‡
- **completion_tokens**: è¾“å‡ºtokenæ•°é‡

## ğŸ”§ é«˜çº§é…ç½®

### è‡ªå®šä¹‰å®¢æˆ·ç«¯é…ç½®

```python
client = VLLMMetricsClient(
    vllm_base_url="http://your-vllm-server:8000",
    jaeger_base_url="http://your-jaeger:16686",
    otlp_endpoint="http://your-otlp-collector:4317"
)
```

### è¯·æ±‚å‚æ•°é€‰é¡¹

```python
metrics = await client.send_request(
    messages=[{"role": "user", "content": "ä½ çš„é—®é¢˜"}],
    model="auto",                    # æ¨¡å‹åç§°ï¼Œ"auto"ä½¿ç”¨é»˜è®¤
    temperature=0.7,                 # æ¸©åº¦å‚æ•°
    max_tokens=100,                  # æœ€å¤§tokenæ•°
    top_p=1.0,                      # Top-pé‡‡æ ·
    stream=True,                     # æ˜¯å¦æµå¼å“åº”
    # å…¶ä»–OpenAIå…¼å®¹å‚æ•°...
)
```

## ğŸ” æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

1. **æ— æ³•è·å–æœåŠ¡ç«¯æŒ‡æ ‡**
   - ç¡®è®¤ vLLM å¯åŠ¨æ—¶å¯ç”¨äº† OpenTelemetry: `--otlp-traces-endpoint`
   - ç¡®è®¤ Jaeger æœåŠ¡æ­£åœ¨è¿è¡Œ: `docker ps | grep jaeger`
   - å¢åŠ  timeout å‚æ•°: `client.enrich_with_server_metrics(metrics, timeout=30)`

2. **è¯·æ±‚å¤±è´¥**
   - æ£€æŸ¥ vLLM æœåŠ¡çŠ¶æ€å’Œåœ°å€
   - ç¡®è®¤æ¨¡å‹å·²æ­£ç¡®åŠ è½½
   - æŸ¥çœ‹é”™è¯¯ä¿¡æ¯: `print(metrics.error_message)`

3. **æŒ‡æ ‡ä¸å‡†ç¡®**
   - æœ¬å·¥å…·ä½¿ç”¨ UUID å’Œ OpenTelemetry ä¿è¯æŒ‡æ ‡å‡†ç¡®æ€§
   - å³ä½¿åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹ä¹Ÿèƒ½æ­£ç¡®åŒºåˆ†æ¯ä¸ªè¯·æ±‚
   - æ— éœ€æ‹…å¿ƒæŒ‡æ ‡æ··åˆé—®é¢˜

### è°ƒè¯•æ¨¡å¼

```python
# æ‰“å¼€è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥ trace æ•°æ®
print(f"Trace ID: {metrics.trace_id}")
print(f"Jaeger URL: http://localhost:16686/trace/{metrics.trace_id}")
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ

1. **ç”Ÿäº§ç¯å¢ƒç›‘æ§**
   - å®šæœŸæ”¶é›†å…³é”®æŒ‡æ ‡(TTFT, é˜Ÿåˆ—æ—¶é—´, ååé‡)
   - è®¾ç½®æŒ‡æ ‡é˜ˆå€¼å‘Šè­¦
   - ç›‘æ§å¹¶å‘åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°

2. **æ€§èƒ½ä¼˜åŒ–**
   - å…³æ³¨é˜Ÿåˆ—ç­‰å¾…æ—¶é—´ï¼Œä¼˜åŒ–å¹¶å‘å¤„ç†
   - ç›‘æ§TTFTï¼Œä¼˜åŒ–æ¨¡å‹åŠ è½½å’Œé¢„å¡«å……
   - åˆ†æTPOT/ITLï¼Œä¼˜åŒ–tokenç”Ÿæˆé€Ÿåº¦

3. **å®¹é‡è§„åˆ’**
   - ä½¿ç”¨å¹¶å‘æµ‹è¯•è¯„ä¼°ç³»ç»Ÿå®¹é‡
   - åŸºäºE2Eå»¶è¿Ÿè§„åˆ’ç”¨æˆ·ä½“éªŒ
   - æ ¹æ®é˜Ÿåˆ—æ—¶é—´è°ƒæ•´èµ„æºé…ç½®

## ğŸ¤ æŠ€æœ¯åŸç†

æœ¬å·¥å…·åŸºäºä»¥ä¸‹æŠ€æœ¯ä¿è¯æŒ‡æ ‡å‡†ç¡®æ€§ï¼š

- **W3C Trace Context**: å›½é™…æ ‡å‡†çš„åˆ†å¸ƒå¼è¿½è¸ªåè®®
- **UUID å”¯ä¸€æ€§**: æ¯ä¸ªè¯·æ±‚éƒ½æœ‰å…¨çƒå”¯ä¸€çš„è¿½è¸ªID
- **OpenTelemetry**: æœåŠ¡ç«¯æŒ‡æ ‡çš„æ ‡å‡†åŒ–æ”¶é›†
- **æµå¼å“åº”å¤„ç†**: å®¢æˆ·ç«¯å®æ—¶è®¡ç®—tokençº§åˆ«æŒ‡æ ‡

è¿™ç¡®ä¿äº†å³ä½¿åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹ï¼Œæ¯ä¸ªè¯·æ±‚çš„æŒ‡æ ‡éƒ½èƒ½å‡†ç¡®å¯¹åº”ï¼Œæ— æ··åˆæˆ–é”™è¯¯å½’å±é—®é¢˜ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ†˜ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„æ•…éšœæ’æŸ¥éƒ¨åˆ†
2. æ£€æŸ¥ Jaeger UI ä¸­çš„è¿½è¸ªæ•°æ®: http://localhost:16686
3. éªŒè¯ vLLM æœåŠ¡çš„ OpenTelemetry é…ç½®

---

**æ³¨æ„**: æœ¬å·¥å…·éœ€è¦ vLLM æœåŠ¡å¯ç”¨ OpenTelemetry æ”¯æŒæ‰èƒ½è·å–å®Œæ•´çš„æœåŠ¡ç«¯æŒ‡æ ‡ã€‚å®¢æˆ·ç«¯æŒ‡æ ‡(TTFT, TPOT, ITL)æ— éœ€é¢å¤–é…ç½®å³å¯å‡†ç¡®æµ‹é‡ã€‚